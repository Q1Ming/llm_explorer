# predict.py (æœ€ç»ˆç‰ˆï¼Œæ”¯æŒåˆ†ç±»ä¸ç”Ÿæˆ)

import argparse
import os
import torch
from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification, AutoModelForCausalLM


def predict_sentiment(text, model, tokenizer):
    """å¤„ç†åˆ†ç±»ä»»åŠ¡çš„é¢„æµ‹é€»è¾‘ã€‚"""
    device = model.device
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_class_id = logits.argmax().item()
    label = model.config.id2label.get(predicted_class_id, f"UNKNOWN_ID_{predicted_class_id}")

    print(f"æ–‡æœ¬: '{text}'")
    if label == "POSITIVE":
        print(">> é¢„æµ‹æƒ…æ„Ÿ: æ­£é¢ ğŸ‘")
    elif label == "NEGATIVE":
        print(">> é¢„æµ‹æƒ…æ„Ÿ: è´Ÿé¢ ğŸ‘")
    else:
        print(f">> é¢„æµ‹ç»“æœ: {label}")
    print("-" * 30)


def generate_text(prompt, model, tokenizer):
    """å¤„ç†ç”Ÿæˆä»»åŠ¡çš„åˆ›ä½œé€»è¾‘ã€‚"""
    device = model.device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # max_length æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æ€»é•¿åº¦
    # no_repeat_ngram_size é¿å…ç”Ÿæˆé‡å¤çš„çŸ­è¯­
    outputs = model.generate(
        **inputs,
        max_length=60,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        pad_token_id=tokenizer.eos_token_id  # é¿å…è­¦å‘Š
    )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"è¾“å…¥: '{prompt}'")
    print(f">> æ¨¡å‹åˆ›ä½œ:\n{generated_text}")
    print("-" * 30)


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨å¾®è°ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚")
    parser.add_argument(
        "model_path",
        type=str,
        help="æŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: outputs/sentiment_analyzer)"
    )
    args = parser.parse_args()

    if not os.path.isdir(args.model_path):
        print(f"é”™è¯¯: æ¨¡å‹ç›®å½• '{args.model_path}' æœªæ‰¾åˆ°ã€‚")
        return

    try:
        print(f"æ­£åœ¨ä» '{args.model_path}' åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        # 1. å…ˆåŠ è½½é…ç½®ï¼Œåˆ¤æ–­æ¨¡å‹ç±»å‹
        config = AutoConfig.from_pretrained(args.model_path)

        # 2. æ ¹æ®æ¨¡å‹æ¶æ„ä¸­çš„`architectures`å­—æ®µæ¥å†³å®šåŠ è½½å“ªä¸ªæ¨¡å‹ç±»
        # è¿™æ˜¯æœ€å¯é çš„æ–¹æ³•ï¼
        model_class = config.architectures[0]

        if "ForSequenceClassification" in model_class:
            print("æ£€æµ‹åˆ°åˆ†ç±»æ¨¡å‹ï¼Œè¿›å…¥æƒ…æ„Ÿåˆ†ææ¨¡å¼...")
            model = AutoModelForSequenceClassification.from_pretrained(args.model_path)
            task_type = "classification"
        elif "ForCausalLM" in model_class:
            print("æ£€æµ‹åˆ°ç”Ÿæˆæ¨¡å‹ï¼Œè¿›å…¥æ–‡æœ¬åˆ›ä½œæ¨¡å¼...")
            model = AutoModelForCausalLM.from_pretrained(args.model_path)
            task_type = "generation"
        else:
            print(f"è­¦å‘Š: æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_class}'ã€‚å°è¯•ä½¿ç”¨ AutoModel åŠ è½½ã€‚")
            model = AutoModel.from_pretrained(args.model_path)
            task_type = "unknown"

        tokenizer = AutoTokenizer.from_pretrained(args.model_path)
        print("åŠ è½½å®Œæˆï¼")

        # 3. å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device.upper()}")

        # 4. æ ¹æ®ä»»åŠ¡ç±»å‹ï¼Œå¯åŠ¨ä¸åŒçš„äº¤äº’ä¼šè¯
        if task_type == "classification":
            print("\næƒ…æ„Ÿåˆ†æé¢„æµ‹å·²å°±ç»ªã€‚è¯·è¾“å…¥ä¸€å¥è¯è¿›è¡Œåˆ†æï¼ˆè¾“å…¥ 'exit' æˆ– Ctrl+C é€€å‡ºï¼‰ï¼š")
            run_interactive_loop(predict_sentiment, model, tokenizer)
        elif task_type == "generation":
            print("\næ–‡æœ¬åˆ›ä½œå·²å°±ç»ªã€‚è¯·è¾“å…¥å¼€å¤´ï¼ˆpromptï¼‰æ¥è®©æ¨¡å‹ç»­å†™ï¼ˆè¾“å…¥ 'exit' æˆ– Ctrl+C é€€å‡ºï¼‰ï¼š")
            run_interactive_loop(generate_text, model, tokenizer)
        else:
            print("æ— æ³•ç¡®å®šäº¤äº’æ¨¡å¼ï¼Œç¨‹åºé€€å‡ºã€‚")

    except Exception as e:
        print(f"å‘ç”Ÿäº†ä¸€ä¸ªæ„å¤–é”™è¯¯: {e}")


def run_interactive_loop(handler_func, model, tokenizer):
    """ä¸€ä¸ªé€šç”¨çš„äº¤äº’å¾ªç¯ã€‚"""
    try:
        while True:
            user_input = input("è¯·è¾“å…¥: ")
            if user_input.lower() == 'exit':
                break
            if not user_input.strip():
                continue
            handler_func(user_input, model, tokenizer)
    except (KeyboardInterrupt, EOFError):
        print("\nå†è§ï¼")


if __name__ == "__main__":
    main()